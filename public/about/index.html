<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <title>DJ Phonetic</title>
  <meta name="description" content="Beatbox with historical speeches">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:title" content="">
  <meta property="og:type" content="">
  <meta property="og:url" content="">
  <meta property="og:image" content="">

  <link rel="apple-touch-icon" href="../icon.png">

  <link rel="stylesheet" href="../css/vendor/normalize.css">
  <link rel="stylesheet" href="../css/about.css">

  <meta name="theme-color" content="#fafafa">
</head>

<body>
  <div id="about" class="page">
    <div class="title">
      <img src="../icon.png" alt="DJ Phonetic logo comprised of an open mouth with a red speaker s a tongue" class="icon" />
      <div class="text">
        <h1>DJ Phonetic</h1>
        <h2>Beatbox with historical speeches</h2>
      </div>
    </div>

    <!-- insert video here -->

    <p>
      <a href="../" class="button">Start playing</a>
    </p>

    <h3>What is DJ Phonetic?</h3>

    <p>DJ Phonetic is a free online tool for making beats with the kicks, snares, and hi-hats that are hidden within human speech.</p>

    <h3>Who made DJ Phonetic?</h3>

    <p>This project was created by <a href="https://brianfoo.com/">Brian Foo</a>, an artist who likes to make creative tools powered by open access cultural heritage material from libraries, archives, and museums. He was the 2020 <a href="https://labs.loc.gov/about/opportunities/innovator-in-residence-program" target="_blank">Innovator in Residence</a> at the Library of Congress where he created <a href="https://citizen-dj.labs.loc.gov/" target="_blank">Citizen DJ</a>, a tool for creating hip hop music using the Library's free-to-use audio and visual material.</p>

    <h3>Why did you create this project?</h3>
    
    <p>I've always been fascinated by the inflections, intonations, tonality, accents, and cadences of human speech, qualities that are often lost through the process of transcription. This project attempts to center rather than obscure these speech qualities by turning them into musical instruments, asking the user to listen carefully for the hidden beats, rhythms, and melodies embedded in human speech.</p>

    <h3>Where do you get your source material from?</h3>

    <p>All the audio recordings used are in the public domain and were sourced from publicly accessible sources such as the <a href="https://www.loc.gov/" target="_blank">Library of Congress</a>'s <a href="https://www.loc.gov/collections/american-english-dialect-recordings-from-the-center-for-applied-linguistics/about-this-collection/" target="_blank">American English Dialect Recordings</a>, <a href="https://www.virginia.edu/" target="_blank">University of Virginia</a>'s <a href="https://millercenter.org/the-presidency/presidential-speeches" target="_blank">Miller Center</a>, and <a href="https://msu.edu/" target="_blank">Michigan State University</a>'s <a href="https://lib.msu.edu/vvl/" target="_blank">G. Robert Vincent Voice Library</a>. The original recordings are linked within the app's interface.</p>

    <h3>How do you choose what parts of speech correspond to different drums?</h3>

    <p>I simply mapped specific <a href="https://en.wikipedia.org/wiki/Phoneme" target="_blank">phonemes</a> (/b/ in "ball", /k/ in "kite", /t/ in "tap") to specific drums (kick, snare, hihat.) This is obviously a subjective process. For reference and inspiration, I looked to <a href="https://en.wikipedia.org/wiki/Beatboxing">beatboxing</a>, the art mimicking drum sounds with one's one's mouth, lips, tongue, and voice. I then used basic audio analysis techniques to select the best drum candidates based on audio characteristics such as audio brightness, sharpness, tonality, and loudness. For example, a snare sound would be loud and short with high sharpness and low tonality.</p>

    <h3>How did you align the transcript to the audio so precisely?</h3>

    <p>I use a tool called the <a href="https://montreal-forced-aligner.readthedocs.io/en/latest/index.html" target="_blank">Montreal Forced Aligner</a> to automatically align the source audio to the transcript with precision down to the phoneme. The Montreal Forced Aligner is built on top of <a href="https://github.com/kaldi-asr/kaldi" target="_blank">Kaldi</a>, an open source speech recognition toolkit.</p>

    <h3>Are the drum sounds really just from speech?</h3>

    <p>Yes, the drums you hear are taken directly from the source material. However, some basic filters (e.g. highpass, lowpass, highshelf, etc) are applied to boost or reduce the bass depending on which drum it is.</p>

    <h3>Can I download what I create?</h3>

    <p>Unfortunately, this feature is not available. This app focuses on playing with human speech to make beats rather than be a full-blown drum machine or DAW. However, you can record your screen while you use DJ Phonetic to capture your performance.</p>

    <h3>Can I download the individual drum sounds?</h3>

    <p>Yes! Simply right-click (or press-and-hold on a touch device) on text within the transcript and click on the download icon. Then you can save an audio clip as a .wav file to your computer which you can use in any music making software.</p>

    <h3>Is this project open source?</h3>

    <p>Yes, the code is open source and can be found in <a href="https://github.com/beefoo/dj-phonetic" target="_blank">this repository</a> along with documentation for using your own audio recordings.</p>

    <h3>I found an error in one of you transcripts</h3>

    <p>Thank you, please send me the error and correction at <a href="mailto:hello@brianfoo.com">hello@brianfoo.com</a>.</p>
  </div>
</body>

</html>
